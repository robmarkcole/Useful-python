{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Face classification using Haar-like feature descriptor\n\n\nHaar-like feature descriptors were successfully used to implement the first\nreal-time face detector [1]_. Inspired by this application, we propose an\nexample illustrating the extraction, selection, and classification of Haar-like\nfeatures to detect faces vs. non-faces.\n\nNotes\n-----\n\nThis example relies on scikit-learn to select and classify features.\n\nReferences\n----------\n\n.. [1] Viola, Paul, and Michael J. Jones. \"Robust real-time face\n       detection.\" International journal of computer vision 57.2\n       (2004): 137-154.\n       http://www.merl.com/publications/docs/TR2004-043.pdf\n       :DOI:`10.1109/CVPR.2001.990517`\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from __future__ import division, print_function\nfrom time import time\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom dask import delayed\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\n\nfrom skimage.data import lfw_subset\nfrom skimage.transform import integral_image\nfrom skimage.feature import haar_like_feature\nfrom skimage.feature import haar_like_feature_coord\nfrom skimage.feature import draw_haar_like_feature"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The usual feature extraction scheme\n##############################################################################\n The procedure to extract the Haar-like feature for an image is quite easy: a\n region of interest (ROI) is defined for which all possible feature will be\n extracted. The integral image of this ROI will be computed and all possible\n features will be computed.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "@delayed\ndef extract_feature_image(img, feature_type, feature_coord=None):\n    \"\"\"Extract the haar feature for the current image\"\"\"\n    ii = integral_image(img)\n    return haar_like_feature(ii, 0, 0, ii.shape[0], ii.shape[1],\n                             feature_type=feature_type,\n                             feature_coord=feature_coord)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will use a subset of the CBCL which is composed of 100 face images and 100\nnon-face images. Each image has been resized to a ROI of 19 by 19 pixels. We\nwill keep 75 images from each group to train a classifier and check which\nextracted features are the most salient, and use the remaining 25 from each\nclass to check the performance of the classifier.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "images = lfw_subset()\n# For speed, only extract the two first types of features\nfeature_types = ['type-2-x', 'type-2-y']\n\n# Build a computation graph using dask. This allows using multiple CPUs for\n# the computation step\nX = delayed(extract_feature_image(img, feature_types)\n            for img in images)\n# Compute the result using the \"processes\" dask backend\nt_start = time()\nX = np.array(X.compute(scheduler='processes'))\ntime_full_feature_comp = time() - t_start\ny = np.array([1] * 100 + [0] * 100)\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=150,\n                                                    random_state=0,\n                                                    stratify=y)\n\n# Extract all possible features to be able to select the most salient.\nfeature_coord, feature_type = \\\n        haar_like_feature_coord(width=images.shape[2], height=images.shape[1],\n                                feature_type=feature_types)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A random forest classifier can be trained in order to select the most salient\nfeatures, specifically for face classification. The idea is to check which\nfeatures are the most often used by the ensemble of trees. By using only\nthe most salient features in subsequent steps, we can dramatically speed up\ncomputation, while retaining accuracy.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Train a random forest classifier and check performance\nclf = RandomForestClassifier(n_estimators=1000, max_depth=None,\n                             max_features=100, n_jobs=-1, random_state=0)\nt_start = time()\nclf.fit(X_train, y_train)\ntime_full_train = time() - t_start\nauc_full_features = roc_auc_score(y_test, clf.predict_proba(X_test)[:, 1])\n\n# Sort features in order of importance, plot six most significant\nidx_sorted = np.argsort(clf.feature_importances_)[::-1]\n\nfig, axes = plt.subplots(3, 2)\nfor idx, ax in enumerate(axes.ravel()):\n    image = images[0]\n    image = draw_haar_like_feature(image, 0, 0,\n                                   images.shape[2],\n                                   images.shape[1],\n                                   [feature_coord[idx_sorted[idx]]])\n    ax.imshow(image)\n    ax.set_xticks([])\n    ax.set_yticks([])\n\nfig.suptitle('The most important features')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can select the most important features by checking the cumulative sum of\nthe feature importance index; below, we keep features representing 70% of the\ncumulative value which represent only 3% of the total number of features.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "cdf_feature_importances = np.cumsum(clf.feature_importances_[idx_sorted])\ncdf_feature_importances /= np.max(cdf_feature_importances)\nsig_feature_count = np.count_nonzero(cdf_feature_importances < 0.7)\nsig_feature_percent = round(sig_feature_count /\n                            len(cdf_feature_importances) * 100, 1)\nprint(('{} features, or {}%, account for 70% of branch points in the random '\n       'forest.').format(sig_feature_count, sig_feature_percent))\n\n# Select the most informative features\nselected_feature_coord = feature_coord[idx_sorted[:sig_feature_count]]\nselected_feature_type = feature_type[idx_sorted[:sig_feature_count]]\n# Note: we could select those features from the\n# original matrix X but we would like to emphasize the usage of `feature_coord`\n# and `feature_type` to recompute a subset of desired features.\n\n# Delay the computation and build the graph using dask\nX = delayed(extract_feature_image(img, selected_feature_type,\n                                  selected_feature_coord)\n            for img in images)\n# Compute the result using the *threads* backend:\n# When computing all features, the Python GIL is acquired to process each ROI,\n# and this is where most of the time is spent, so multiprocessing is faster.\n# For this small subset, most of the time is spent on the feature computation\n# rather than the ROI scanning, and using threaded is *much* faster, because\n# we avoid the overhead of launching a new process.\nt_start = time()\nX = np.array(X.compute(scheduler='threads'))\ntime_subs_feature_comp = time() - t_start\ny = np.array([1] * 100 + [0] * 100)\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=150,\n                                                    random_state=0,\n                                                    stratify=y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Once the features are extracted, we can train and test the a new classifier.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "t_start = time()\nclf.fit(X_train, y_train)\ntime_subs_train = time() - t_start\n\nauc_subs_features = roc_auc_score(y_test, clf.predict_proba(X_test)[:, 1])\n\nsummary = (('Computing the full feature set took {:.3f}s, plus {:.3f}s '\n            'training, for an AUC of {:.2f}. Computing the restricted feature '\n            'set took {:.3f}s, plus {:.3f}s training, for an AUC of {:.2f}.')\n           .format(time_full_feature_comp, time_full_train, auc_full_features,\n                   time_subs_feature_comp, time_subs_train, auc_subs_features))\n\nprint(summary)\nplt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}